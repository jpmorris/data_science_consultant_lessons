\documentclass[preview]{standalone}
\usepackage{amsmath}
\usepackage{graphicx}

\begin{document}

\begin{table}[ht]
\centering
\textbf{Gradient Decent Pseudocode} \\
\begin{tabular}{|l|l|}
\textbf{Step} & \textbf{Description} \\
\hline
1 & Initialize parameters $\theta_0, \theta_1, \dots, \theta_n$ randomly or with zeros \\
2 & Set the learning rate $\alpha$ and number of iterations $T$ \\
3 & For each iteration $t = 1$ to $T$: \\
4 & \hspace{1cm} Compute the cost function: $J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2$ \\
5 & \hspace{1cm} For each parameter $\theta_j$ (for $j = 0, 1, \dots, n$): \\
6 & \hspace{2cm} Compute the gradient: \\
7 & \hspace{2cm} $\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) \cdot x_j^{(i)}$ \\
8 & \hspace{2cm} Update the parameter: $\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$ \\
9 & Repeat steps 4-8 until convergence or for $T$ iterations \\
10 & Return the optimized parameters $\theta$ \\
\hline
\end{tabular}
\end{table}
\end{document}