# Sentiment Analysis with Logistic Regression

## Review of Supervised ML

![Supervised ML](images/supervised_ml_review.png)

(Notice the parameters, theta, here. This is an example of the parametric/nonparametric distinction.
There are other examples of nonparametric models in ML, such as KNN and certain types of decision
trees like CART and C4.)

## Feature Extraction

We need to vectorize the text data. We can simply one-hot encode the words in the text. If the word
appears it gets a 1; if not, a zero. This representation is very sparse and has large training and
prediction time as the vocabulary increases.

![Feature Extraction](images/feature_extraction.png)

## Frequency Dictionary

In order to represent text, tweets in this case, we convert the occurrence of words in positive and
negative tweets into a frequency dictionary.

![Frequency Dictionary](images/frequency_dictionary_example.png)

## Feature Extraction

We can extract features from the text by summing the frequency in the positive and negative
dictionaries to produce a 3-dimensional vector for a given tweet.

![Feature Extraction](images/feature_extraction_example.png)

## Preprocessing

### Stopwords

Stopwords are words that are filtered out before or after processing of text. They are usually the
most common words in a language and do not add much meaning to the text.

![Stopwords](images/stopwords_example.png)

### Stemming

Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base
or root form.

![Stemming](images/stemming_example.png)

# Review of Logistic Regression

![Logistic Regression](images/logistic_regression_overview.png)

![Logistic Regression Training](images/logistic_regression_training.png)

![Logistic Regression Testing](images/logistic_regression_testing.png)

# Logistic Regression for scratch in R

```r
train <- function(data, labels, alpha){
    iter <- 1
    # set random values for theta
    n <- length(data)
    d <- nrow(data)
    theta <- runif(d, -1, 1)
    old_theta <- rep(1, d)
    # while all thetas are greater than threshold
    while(any(abs(old_theta - theta) > 0.001)){
        # shuffle
        for(i in sample(x=1:n, replace=FALSE)){
            old_theta <- theta
            subtract <- (labels[[i]] * data[[i]])/(1 + exp(-labels[[i]]*drop(theta %*% data[[i]])))
            theta <- theta - alpha * subtract
        }
        iter <- iter + 1
        if(iter > 100){
            print("early exit")
            return(theta)
        }
    }
    return(theta)
}

```

# Quick Review of NLP Logistic Regression
# 

