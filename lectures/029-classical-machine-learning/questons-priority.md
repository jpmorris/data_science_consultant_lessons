1. Data Preprocessing
   - Cleaning Data
   - Feature Selection
     - Why do statisticians look down on stepwise regression
   - Feature Engineering
   - Questions
     - [x] Algorithms for automatic model selection -
           https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection
     - [x] Satistics vs Machine learning (two cultures) -
           https://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning
     - [x] What is the difference between test set and validation set -
           https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set
     - [x] Bayesian and frequentist reasoning in plain english -
           https://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english
     - [x] How to normalize data 0 -1 range -
           https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range
     - [x] How should I transform non-negative data including zeros -
           https://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros
     - [x] How to know that your machine learning problem is hopeless -
           https://stats.stackexchange.com/questions/222179/how-to-know-that-your-machine-learning-problem-is-hopeless
     - [x] How to understand covariance -
           https://stats.stackexchange.com/questions/18058/how-would-you-explain-covariance-to-someone-who-understands-only-the-mean
     - [x] How to determine which distribution fits my data -
           https://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best
     - [x] Central limit theorem -
           https://stats.stackexchange.com/questions/3734/what-intuitive-explanation-is-there-for-the-central-limit-theorem
     - [x] Are large datasets inappropriate for hypothesis testing -
           https://stats.stackexchange.com/questions/2516/are-large-data-sets-inappropriate-for-hypothesis-testing
     - [x] Normalization and Standardization difference -
           https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization
     - [x] Are unbaanced dataset probelmatic and how does oversampling help? -
           https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he
     - [x] when is linear regression machine learning -
           https://stats.stackexchange.com/questions/268755/when-should-linear-regression-be-called-machine-learning
     - [x] train/test/validation set splitting -
           https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn
     - [x] how big is big data -
           https://datascience.stackexchange.com/questions/19/how-big-is-big-data
     - [x] in supervised learning why is it bad to have correlated features -
           https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features
     - [x] always better to use the full dataset to train final model -
           https://datascience.stackexchange.com/questions/33008/is-it-always-better-to-use-the-whole-dataset-to-train-the-final-model
     - [x] Should I go for balanced or represenative datset -
           https://datascience.stackexchange.com/questions/810/should-i-go-for-a-balanced-dataset-or-a-representative-dataset
     - [x] What is groudn truth -
           https://datascience.stackexchange.com/questions/17839/what-is-ground-truth
     - [ ] encoding features like month and hour as categorical -
           https://datascience.stackexchange.com/questions/17759/encoding-features-like-month-and-hour-as-categorial-or-numeric
     - [ ] shoudl we apply normalizaton to test data as well
     - [ ] data normalization before or after train-test split -
2. Supervised Learning - Classification
   - Review of Scikit-learn
   - Review of CARET
   - Review of SAS
   - Questions
     - Explaining why bootstrapping works -
       https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works
     - What is the difference between likelihood and probability -
       https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability
     - Difference between logic and probit models - https://stats.stackexchange.com/questions/20523/
       difference-between-logit-and-probit-models
     - Bagging, boosting, and stacking in machine learning -
       https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning
     - How to deal with perfect separation in logistic regresison -
       https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression
     - What is the influence of C in SVMs - https://staknowledge from random forest -
       https://stats.stackexchange.com/questions/21152/obtaining-ts.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel
     - How to intuitively explain what a kernel is -
       https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is
     - Gradient Boosting Tree vs Random Forest -
       https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest
     - Obtaining knowledge from random forest -
       https://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest
     - Whats the differnce between linear regression and logiestic regression -
       https://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regression
     - Softmax vs sigmoid function in logistic classifier -
       https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier
     - when to use one hot encoding vs labelencoder vs dictvectorizer -
       https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor
     - are svms till stat of the art -
       https://datascience.stackexchange.com/questions/711/are-support-vector-machines-still-considered-state-of-the-art-in-their-nichekexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent
   - Why is overfitting bad in ml - https://datascience.stac
     - What does logits in machine learning mean -
       https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean
     - Does Scikit-learn have a forward selection stepwise regression algorithm? -
       https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm
     - when to use random forest over SVM and vice versa -
       https://datascience.stackexchange.com/questions/6838/when-to-use-random-forest-over-svm-and-vice-versa
     - why do we need xbgboost and random forest -
       https://datascience.stackexchange.com/questions/23789/why-do-we-need-xgboost-and-random-forest
     - why use both validation and test set -
       https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set
3. Supervised Learning - Regression

   - Review of Scikit-learn
   - Review of CARET
   - Review of SAS
   - Questions
     - When conduccting multiple regression when do you center and when standardize -
       https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia
     - In linear regression when use log of var -
       https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va
     - When is it ok to remove the intercept in a linear regression model -
       https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model
     - how to force weights to be non-negative in linear regression -
       https://datascience.stackexchange.com/questions/18258/how-to-force-weights-to-be-non-negative-in-linear-regression

4. Unsupervised Learning - Clustering
   - Questions
     - How to understand the drawbacks of k-means -
       https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means
     - Difference between K-means and k-nearest neighbors -
       https://stats.stackexchange.com/questions/56500/what-are-the-main-differences-between-k-means-and-k-nearest-neighbours
     - expectation maximization -
       https://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization
     - k-means clutering for mixed data -
       https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data
     - is it necessary to standardize your data before clustering -
       https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering
5. Unsupervised Learning - Dimensionality Reduction

   - Questions
     - kmeans relation with PCA -
       https://stats.stackexchange.com/questions/183236/what-is-the-relation-between-k-means-clustering-and-pca
     - Making sense of PCA, eigenvectors and eigenvalues -
       https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
     - Relationship between SVD and PCA -
       https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
     - Difference between factor analysis and PCA -
       https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi
     - Can PCA be used with categorical -
       https://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont
     - PCA on correlation or covariance -
       https://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance
     - Reverse PCA -
       https://stats.stackexchange.com/questions/229092/how-to-reverse-pckexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent
   - Why is overfitting bad in ml -
     https://datascience.staca-and-reconstruct-original-variables-from-several-principal-com
     - Why do we need to normalize before applying PCA -
       https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca
     - dimensionality reduction -
       https://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti
     - LDA vs HDP -
       https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process

6. Model Evaluation and Selection

   - ROC vs precision-and-recall curves -
     https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves
   - Why is accuracy not the best measure for assessing classification models -
     https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models
   - Lasso vs ridge - https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge
   - Choice of K in K-fold cross-validation -
     https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
   - Objective function, cost fucntion, loss function -
     https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing
   - Removing variables before PCA -
     https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca
   - Maximum Likelihood Estimation in laymans terms -
     https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms
   - Why deos the Lasso provide variable selection -
     https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection
   - Mean absolute error or root mean error
     https://stats.stackexchange.com/questions/48267/mean-absolute-error-or-root-mean-squared-error
   - multiclass confusion matrix -
     https://stats.stackexchange.com/questions/51296/how-do-you-calculate-precision-and-recall-for-multiclass-classification-using-co
   - best way to remember sensitivyt etc
     https://stats.stackexchange.com/questions/122225/what-is-the-best-way-to-remember-the-difference-between-sensitivity-specificity
   - When is a mdoel underfitted -
     https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted
   - What is the difference between bootstraping and CV -
     https://datascience.stackexchange.com/questions/32264/what-is-the-difference-between-bootstrapping-and-cross-validation
   - Advnages of using AUC over accuracy -
     https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy
   - Difference between gradient descent and stochastic gradient descent -
     https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent
   - Why is overfitting bad in ml -
     https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad-in-machine-learning
   - Model parameters vs hyperparametrs -
     https://datascience.stackexchange.com/questions/14187/what-is-the-difference-between-model-hyperparameters-and-model-parameters
   - Does gradient descent always converge to an
     optimum -https://datascience.stackexchange.com/questions/24534/does-gradient-descent-always-converge-to-an-optimum
   - When is precision more important over recall -
     https://datascience.stackexchange.com/questions/30881/when-is-precision-more-important-over-recall
   - How to choose a model after k-fold cross-validation -
     https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation
   - [] Correlation and covariance -
     https://stats.stackexchange.com/questions/18082/how-would-you-explain-the-difference-between-correlation-and-covariance

7. Advanced Topics
   - Ensemble Methods
   - Feature Engineering
