{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340cae56",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# LLMS and the Generative AI Lifecycle\n",
    "Outline:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6df8f6",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "- **Foundation Models**: ML model that is trained on broad data such that it can be applied across a wide range of uses cases\n",
    "  - in LLM space: GPT, BERT, FLAN-T5, PaLM, LLaMa, BLOOM\n",
    "- **Fine-tuning**: process of training a model on a specific dataset to create a model that is specialized for a particular use case\n",
    "- **Prompt**: the input to a language model\n",
    "- **Context Window**: memory available to prompt\n",
    "- **Inference**: the process of generating output from a model\n",
    "- **Completion**: the output of a language model\n",
    "\n",
    "![Graph Examples](images/prompts_and_completions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e3a24",
   "metadata": {},
   "source": [
    "# LLM use cases:\n",
    "- **Essay Writing**: generating an essay from a prompt\n",
    "- **Text Summarization**: generating a summary of a text\n",
    "- **Text Translation**: translating text from one language to another\n",
    "- **Natural Language to Programming Language**: translating natural language to programming language\n",
    "- **Entity Extraction**: extracting entities from text\n",
    "- **Connecting to APIs**: extending the model with information from other systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabc710",
   "metadata": {},
   "source": [
    "Aside: programming as requirements specification\n",
    "- Every few lines of code in a program specifies a explict or implicit requirement of a program\n",
    "- When code is generated there can be an assumption of the requirement or the requirement needs to be explicit\n",
    "Question: with the idealized LLM, what is the minimum number of English prompts it would take to generate the Windows Oparating System?\n",
    "Question: is there a future world without programmers in python? Maybe programmers just use English?\n",
    "Bigger Question: is there some difficulty that arises between the race of model parameter size and the context window size?\n",
    "Solution?: don't build Windows, build windows components? However this world still needs system-level designers.\n",
    "To think about:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfb48c",
   "metadata": {},
   "source": [
    "# Generating text before LLM\n",
    "- RNN (Recurrent Neural Network) and LSTM (Long Short-Term Memory) networks.\n",
    "\n",
    "![Graph Examples](images/prompts_and_completions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78528a22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 2017 - 'Attention is All You Need' paper\n",
    "- **Transduction**: reasoning from observed cases to specific cases\n",
    "- **Induction**: reasoning from observed cases to general rule and applied to specific cases\n",
    "- **Attention**: calculating soft weights in the context window\n",
    "  - Sequential computation in RNNs and LSTMs\n",
    "  - Parallel computation in Transformers\n",
    "\n",
    "- AIAYN Paper\n",
    "  - Google Researchers\n",
    "  - Introduces the Transformer architecture\n",
    "  - Only 10 pages\n",
    "  - Currently in March 2024 has 7019 citations\n",
    "\n",
    "![Attention is all you Need](images/attention_is_all_you_need.png)\n",
    "\n",
    "![Transformer Architecture](images/transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7ab02",
   "metadata": {},
   "source": [
    "# Attention:\n",
    "- **Attention map**: a matrix that shows the relationship between the input and output of a model\n",
    "\n",
    "![Attention Map](images/attention_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228bdb3",
   "metadata": {},
   "source": [
    "# Transformer:\n",
    "- Encoder-Decoder architecture\n",
    "  - Encoder: encodes inputs ('prompts') with contextual understanding and produces one vector per input token\n",
    "  - Decoder: Accepts input tokens and generates new tokens\n",
    "- Tokenizer method must be same for training and inference\n",
    "- Embedding layer: converts tokens to vectors\n",
    "  - can relate words in embedding space and calculate distance between words (cosine similarity)\n",
    "- Positional Encoding: adds information about the position of the token in the sequence\n",
    "- Multi-head attention: allows the model to focus on different parts of the input\n",
    "- Feed-forward network: a network where information flows forward (as opposed to recurrent networks for example)\n",
    "- Softmax: a function that converts a vector of numbers into a probability distribution\n",
    "\n",
    "\n",
    "![Embedding Space](images/embedding_space.png)\n",
    "\n",
    "![Transfomer Simplified](images/transfomer_simplified.png)\n",
    "\n",
    "![Transfomer Translation Example](images/transformer_translation_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1552a",
   "metadata": {},
   "source": [
    "# Architecture Variations:\n",
    "- Encoder-only transformer: input and output sequence are same length\n",
    "  - Can be used for classificaiton tasks, sentiment analysis\n",
    "  - e.g. BERT\n",
    "- Encoder-decoder transformer: used for tasks like translation\n",
    "  - e.g. T5, BART,\n",
    "  - sequence-to-sequence models\n",
    "  - Can be used to text generation task\n",
    "- Decoder-only transformer: used for tasks like language modeling\n",
    "  - Most commonly used\n",
    "  - e.g. GPT famiy, BLOOM, Jurassic, LLaMa\n",
    "  - Can be generalized for most tasks\n",
    "\n",
    "![Transfomer Architecture Variations](images/transformer_architecture_variation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23554514",
   "metadata": {},
   "source": [
    "# Prompt Engineering:\n",
    "- **Prompt Engineering**: the process of designing prompts to get the desired output from a model\n",
    "- **In-context learning**: training a model on a specific dataset to improve performance on a specific task\n",
    "  - **zero-shot inference**: training a model on a specific dataset to improve performance on a specific task\n",
    "\n",
    "![In-context learning zero shot](images/icl_zero_shot.png)\n",
    "\n",
    "![In-context learning one shot](images/icl_one_shot.png)\n",
    "\n",
    "![In-context learning few shot](images/icl_few_shot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebe8a8",
   "metadata": {},
   "source": [
    "# Ways to improve LLM performance:\n",
    "- **Prompt Engineering**: designing prompts to get the desired output\n",
    "- **In-context learning**: training a model on a specific dataset to improve performance on a specific task\n",
    "- **Inference Parameters**: adjusting parameters to improve performance\n",
    "  - Not prameters or hypterprameters learned in training, but parameters that are set during inference\n",
    "- **Fine-tuning**: training a model on a specific dataset to improve performance on a specific task\n",
    "- **Model Size**: larger models tend to perform better\n",
    "- **RAG (Retrieval Augmented Generation)**: combining generative and retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df7056",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Ways to modify LLM output - Inference Prameters:\n",
    "- - Examples:\n",
    "    - **Top-p sampling**: samples from the top p tokens (where p is a probability threshold the tokens do not exceed)\n",
    "    - **Top-k sampling**: samples from the top k tokens (where k is a number of tokens to sample from)\n",
    "    - **Temperature**: controls the randomness of the output, by controling the shape of the softmax distribution\n",
    "      - acutally modifies the probabilities as opposed to p or k sampling\n",
    "\n",
    "![Generative config - Greedy vs Random sampling](images/greedy_v_random.png)\n",
    "\n",
    "![Top-p, Top-k sampling](images/top_p_top_k.png)\n",
    "\n",
    "![Temperature](images/temperature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92be13",
   "metadata": {},
   "source": [
    "# Generative AI Lifecycle:\n",
    "- Define the use case -  define the problem you want to solve, as specifically and narrowly as possible\n",
    "- Choosing an existing model or pretrain your own\n",
    "- Prompt Engineering - design prompts to get the desired output\n",
    "- Fine-tuning - train the model on a specific dataset to improve performance\n",
    "- Align with human feedback\n",
    "- Evaluate the model\n",
    "- Optimize and deploy\n",
    "- Augment model and build LLM-powered applications\n",
    "\n",
    "\n",
    "![Genernative AI Lifecycle](images/generative_ai_lifecycle.png)\n",
    "\n",
    "![AI Tasks](images/ai_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f960dae",
   "metadata": {},
   "source": [
    "references:\n",
    "https://www.coursera.org/learn/generative-ai-with-llms/lecture/9uWab/course-introduction"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
